{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# hugging face tokenizers\n",
    "参考：https://huggingface.co/docs/tokenizers/pipeline\n",
    "## normalizer\n",
    "- 使原始语料更加“干净”"
   ],
   "id": "fc8961674ad42b91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T05:49:19.226578Z",
     "start_time": "2025-03-23T05:49:19.223785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------- normalizer ---------------------------------- #\n",
    "\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "\n",
    "normalizer.normalize_str(\"Héllò hôw are ü?\")\n",
    "# \"Hello how are u?\""
   ],
   "id": "82dd649cc30db569",
   "outputs": [
    {
     "data": {
      "text/plain": "'Hello how are u?'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# tokenizer中可以指定normalizer\n",
    "# tokenizer.normalizer = normalizer"
   ],
   "id": "8cc686b03d304e59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## pre-tokenizer\n",
    "- 预分词器，会将文本分割为最小的token单位。之后vocab_size不会超过此时的分词数量\n",
    "    - 比如利用BPE会合并某些token成为一个新token，vocab_size就变小了"
   ],
   "id": "b5afd470dacedb81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:30:19.945311Z",
     "start_time": "2025-03-17T07:30:19.941234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "pre_tokenizer = Whitespace()\n",
    "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")\n",
    "# [(\"Hello\", (0, 5)), (\"!\", (5, 6)), (\"How\", (7, 10)), (\"are\", (11, 14)), (\"you\", (15, 18)),\n",
    "#  (\"?\", (18, 19)), (\"I\", (20, 21)), (\"'\", (21, 22)), ('m', (22, 23)), (\"fine\", (24, 28)),\n",
    "#  (\",\", (28, 29)), (\"thank\", (30, 35)), (\"you\", (36, 39)), (\".\", (39, 40))]"
   ],
   "id": "82ee9c560eb9210a",
   "outputs": [
    {
     "data": {
      "text/plain": "[('Hello', (0, 5)),\n ('!', (5, 6)),\n ('How', (7, 10)),\n ('are', (11, 14)),\n ('you', (15, 18)),\n ('?', (18, 19)),\n ('I', (20, 21)),\n (\"'\", (21, 22)),\n ('m', (22, 23)),\n ('fine', (24, 28)),\n (',', (28, 29)),\n ('thank', (30, 35)),\n ('you', (36, 39)),\n ('.', (39, 40))]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 预分词器会输出每个token的开始位置和结束位置\n",
    "- Whitespace会以 \\[空格, tab, 回车\\] 等空白字符为间隔分词\n",
    "\n",
    "可以利用pre_tokenizers.Sequence组合多个预分词器，预分词器会按照顺序执行"
   ],
   "id": "112c8b5da7c15ded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T07:48:55.620412Z",
     "start_time": "2025-02-25T07:48:55.615052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Digits\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)]) # 可以按顺序组合多个预分词器\n",
    "pre_tokenizer.pre_tokenize_str(\"Call 911!\")\n",
    "# [(\"Call\", (0, 4)), (\"9\", (5, 6)), (\"1\", (6, 7)), (\"1\", (7, 8)), (\"!\", (8, 9))]"
   ],
   "id": "e66b5a99533b7253",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call', (0, 4)), ('9', (5, 6)), ('1', (6, 7)), ('1', (7, 8)), ('!', (8, 9))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# tokenizer指定预分词器\n",
    "# tokenizer.pre_tokenizer = pre_tokenizer"
   ],
   "id": "cfc94db4f0c9594"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## model\n",
    "- 分别有4类模型：\n",
    "    - models.BPE\n",
    "    - models.Unigram\n",
    "    - models.WordLevel\n",
    "    - models.WordPiece\n",
    "\n",
    "## post-processing\n",
    "- 添加其他特殊token"
   ],
   "id": "c5885494bbf9bd31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 添加额外token使其变成满足BERT模型的输入\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a BERT tokenizer from scratch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# -------------------------------- model ---------------------------------- #\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T08:32:39.854463Z",
     "start_time": "2025-03-17T08:32:39.845941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# -------------------------------- normalizer ---------------------------------- #\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-17T08:32:45.175917Z",
     "start_time": "2025-03-17T08:32:45.170341Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
